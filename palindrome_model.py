import torch
import torch.nn as nn
import torch.utils.data as data
import torch.optim as optim

_global_words_0 = ['аа', 'аатаа', 'аба', 'абба', 'абиба', 'ава', 'аваава', 'авава', 'авва', 'ага', 'агга', 'ада', 'адда', 'ажа', 'ака', 'акака', 'аканака', 'акика', 'акка', 'ала', 'алала', 'алафала', 'амакама', 'амма', 'ана', 'анапана', 'анасана', 'анатана', 'анисина', 'анна', 'анона', 'апа', 'апипа', 'аппа', 'ара', 'арара', 'арора', 'арра', 'арура', 'аса', 'ата', 'атета', 'атта', 'аулуа', 'афа', 'аха', 'ахаха', 'ахоха', 'ахха', 'аца', 'ацыца', 'ача', 'аша', 'баб', 'бараб', 'батаб', 'бахаб', 'биб', 'боб', 'вёв', 'вив', 'вызыв', 'гачаг', 'гег', 'гиг', 'гириг', 'гог', 'гыг', 'гэг', 'дед', 'дид', 'диороид', 'довод', 'дойод', 'дород', 'доход', 'дуд', 'еае', 'ейе', 'ёре', 'ере', 'ёте', 'жож', 'заказ', 'замаз', 'заз', 'зуз', 'чи', 'и', 'иааи', 'иаи', 'иби', 'иви', 'идииди', 'ижжи', 'изи', 'ики', 'или', 'илли', 'иньни', 'ири', 'ирори', 'ихи', 'ичи', 'ичиичи', 'ишши', 'йай', 'йой', 'каак', 'кабак', 'кавак', 'казак', 'кайак', 'как', 'канак', 'капак', 'карак', 'касак', 'кассак', 'кек', 'кёк', 'келек', 'келлек', 'керек', 'кесек', 'кечёк', 'кибик', 'кижик', 'кизик', 'киик', 'кийик', 'кик', 'килик', 'киллилиллик', 'киник', 'киноник', 'кичик', 'кишик', 'ковок', 'кок', 'коллок', 'колок', 'комок', 'конок', 'копок', 'коппок', 'корок', 'косок', 'кошок', 'кудук', 'кук', 'кумук', 'курук', 'куук', 'кыйык', 'кык', 'кытык', 'кэк', 'кюк', 'лаал', 'лал', 'лобол', 'лол', 'лыл', 'мадам', 'мазам', 'макам', 'мам', 'манам', 'марам', 'мелем', 'мем', 'мивим', 'мидим', 'миллим', 'мим', 'миним', 'мом', 'моном', 'мум', 'мурум', 'мэм', 'наан', 'набан', 'наган', 'назан', 'накан', 'нан', 'напан', 'насан', 'нашан', 'некен', 'нен', 'ненен', 'нигин', 'нимин', 'нойон', 'нон', 'ноон', 'норурон', 'нэн', 'нян', 'о', 'обибо', 'обо', 'ово', 'оддо', 'ойо', 'око', 'оло', 'ололо', 'оно', 'оо', 'оро', 'ороборо', 'оруро', 'оссо', 'офо', 'очо', 'ошо', 'переп', 'покоп', 'поп', 'посоп', 'потоп', 'пуп', 'радар', 'расар', 'ревер', 'реер', 'рейер', 'ремер', 'репер', 'реппер', 'рер', 'рогор', 'ророр', 'ротатор', 'ротор', 'рэпєр', 'рэппєр', 'сас', 'секес', 'сиис', 'солос', 'соссос', 'статс', 'суккус', 'сукус', 'сус', 'таат', 'такат', 'таннат', 'тартрат', 'тассат', 'тат', 'тауат', 'тидит', 'тиллит', 'тимит', 'тирит', 'тит', 'тихит', 'тозот', 'топот', 'торот', 'тумут', 'тут', 'тыыт', 'у', 'убу', 'уду', 'улу', 'уруушу', 'фараф', 'феф', 'ханнах', 'хенех', 'хох', 'целец', 'чаач', 'чабач', 'чавач', 'чагач', 'чепеч', 'чеч', 'чижич', 'шабаш', 'шалаш', 'шамаш', 'шараш', 'шереш', 'шириш', 'шиш', 'шош', 'шугуш', 'шумуш', 'щэщ', 'эвэ', 'эдэ']
_global_words_1 = ['сарпиночник', 'контрабандист', 'мопед', 'вульгарность', 'ятрышник', 'следопыт', 'оперирование', 'шпажист', 'англосаксонец', 'натуралистичность', 'серница', 'раздел', 'памятник', 'антрополог', 'новорождённая', 'окрол', 'гальваноскоп', 'кофта', 'председатель', 'ржанище', 'помилованная', 'примирение', 'суберин', 'папуаска', 'злободневность', 'эпископ', 'неучтивость', 'адат', 'подавание', 'походка', 'хорь', 'брейд-вымпел', 'предпочтение', 'слепушонок', 'кудель', 'эдикт', 'разнеженность', 'духанщик', 'вертолётчица', 'светотехника', 'провозгласитель', 'бериллий', 'пискунья', 'отгонщик', 'глиптодонт', 'локомобиль', 'пресмыкание', 'старобытность', 'двупланность', 'лютеций', 'прирез', 'рявкание', 'перегрузка', 'токсиколог', 'искусительница', 'дикция', 'древность', 'сертификация', 'магистраль', 'фагоцитоз', 'всесторонность', 'армада', 'люэс', 'бутоньерка', 'полустишие', 'сельхозинвентарь', 'огранка', 'минускул', 'монотипист', 'дань', 'бармен', 'выпирание', 'противосияние', 'альтист', 'бекас', 'глиптотека', 'полиграфия', 'уменьшение', 'лункование', 'клирос', 'пагода', 'элементарность', 'предпочтительность', 'горицвет', 'ксилофон', 'игиль', 'паратость', 'ножовщик', 'гель', 'непроизносимость', 'отшвыривание', 'новолуние', 'обрезок', 'технеций', 'самбист', 'инсулин', 'бирманка', 'гвардия', 'папуас', 'оживание', 'заскабливание', 'переливт', 'кройка', 'контроверза', 'ниспровержение', 'нагреватель', 'плата', 'паралитик', 'платан', 'эндокард', 'скликание', 'инвенция', 'раскутывание', 'загустение', 'чека', 'перенагревание', 'припыл', 'тенётчик', 'натюрморист', 'цивилизованность', 'упрощение', 'отопленец', 'свечка', 'предплужник', 'юродивая', 'неприличность', 'вех', 'лежание', 'драчливость', 'фидеист', 'дезодорант', 'прокапчивание', 'сбережение', 'посыльная', 'фольклористика', 'вдохновитель', 'культурница', 'виноградарь', 'пряничник', 'практикант', 'тузлук', 'плач', 'вареник', 'рислинг', 'транш', 'укупорщик', 'усложнение', 'фальшивомонетничество', 'пышность', 'подстановка', 'санитар', 'линовальщик', 'септик', 'пережидание', 'фалл', 'наивность', 'метафизичность', 'вычищение', 'лярд', 'передрессировывание', 'долгожитель', 'метрополитен', 'прошивка', 'подчитывание', 'ёлка', 'подкрутка', 'аил', 'концепция', 'обмол', 'обиженная', 'жертвенник', 'отчизна', 'шёпот', 'обмыливание', 'водохранилище', 'пантовар', 'притачка', 'кардиография', 'навинчивание', 'угнетённость', 'высокопарность', 'ломаная', 'непоследовательность', 'дилетант', 'разгром', 'горло', 'коалиция', 'федералист', 'отдыхающий', 'неудовлетворительность', 'театральность', 'шурфование', 'подгрузка', 'привкус', 'крольчатина', 'ярка', 'декабристка', 'неоклассик', 'откус', 'педфак', 'одежда', 'евпатория', 'индонезия', 'кастрюля', 'качели', 'мамонт', 'копье', 'колледж', 'авиаметеостанция', 'гороскоп', 'марево', 'десница', 'мозоль', 'копоть', 'креветка', 'качалка', 'конвейер', 'алоэ', 'камбуз', 'катализатор', 'ладонь', 'крыло', 'кий', 'амфибия', 'бородавка', 'кафтан', 'стул', 'иордания', 'электричка', 'пещера', 'мундир', 'водоросль', 'бар', 'балерина', 'граната', 'брус', 'купальня', 'башмачок', 'берлин', 'жеребец', 'воробей', 'сова', 'леденец', 'арена', 'узел', 'софа', 'утюг', 'ландыш', 'вакцина', 'бурьян', 'погреб', 'душ', 'гамбург', 'джунгли', 'голень', 'желток', 'лохмотья', 'берег', 'голгофа', 'шкатулка', 'венок', 'малыш', 'кемпинг', 'паркет', 'баня', 'департамент', 'боекомплект', 'канзас', 'дренаж', 'капсула', 'автомагистраль', 'антиквар', 'мотор', 'карамелька', 'лев', 'впадина', 'декада', 'масленка', 'медпункт', 'мультфильм', 'лотерея', 'калория', 'говядина', 'камфара', 'зубок', 'лимузин', 'бильярд', 'колдобина', 'иероглиф', 'воск', 'шпаргалка', 'траншея', 'авиастроитель', 'пряник', 'бром', 'автопоезд', 'кортик', 'дыхание', 'империя', 'плов']

# сюда копируйте класс WordsDataset, созданный на предыдущем занятии
class WordsDataset(data.Dataset):
    def __init__(self, batch_size=8): # инициализатор класса
        self.words = sorted(_global_words_0 + _global_words_1, key=lambda x: len(x))
        self.batch_size = batch_size

        _text = "".join(_global_words_0 + _global_words_1).lower()
        self.alphabet = set(_text)  # набор символов
        self.alpha_len = len(self.alphabet)
        self.int_to_alpha = dict(enumerate(sorted(self.alphabet)))  # число в символ
        self.alpha_to_int = {b: a for a, b in self.int_to_alpha.items()}  # символ в число
        # здесь код, относящийся к инициализатору
        self.is_palindrome = [0 if word in _global_words_0 else 1 for word in self.words]
        self.dataset_len = len(self.words)

    def __getitem__(self, item): # формирование и возвращение батча данных по индексу item
        # здесь код, относящийся к __getitem__
        eye = torch.eye(len(self.alphabet))
        zero = torch.zeros(len(self.alphabet)).view(1, -1)
        eye = torch.cat((eye, zero), dim=0)

        start = self.batch_size * item
        end = min(self.batch_size * (item + 1), self.dataset_len)
        max_len = len(self.words[end - 1])

        numbers = [[self.alpha_to_int[word[i]] if i < len(word) else self.alpha_len for i in range(max_len)]
                   for word in self.words[start:end]]
        one_hot = torch.stack([eye[n] for n in numbers])
        return one_hot, torch.tensor(self.is_palindrome[start:end])

    def __len__(self): # возврат размер обучающей выборки в батчах
        # здесь код, относящийся к __len__
        last = 0 if self.dataset_len % self.batch_size == 0 else 1
        return self.dataset_len // self.batch_size + last

# здесь объявляйте класс модели
class WordsModel(nn.Module):
    def __init__(self):
        super(WordsModel, self).__init__()
        self.rnn = nn.RNN(input_size=34, hidden_size=16, num_layers=1, batch_first=True, bidirectional=True)
        self.linear = nn.Linear(32, 1)

    def forward(self, x):
        _, h = self.rnn(x)
        hh = torch.cat((h[-2, :, :], h[-1, :, :]), dim=1)
        y = self.linear(hh)
        return y


d_train = WordsDataset(batch_size=8)
train_data = data.DataLoader(d_train, batch_size=1, shuffle=True)

model = WordsModel()
# создание модели с числом входов, равным размеру словаря (размеру one-hot векторов)

optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)
# оптимизатор Adam с шагом обучения 0.01 и параметром weight_decay=0.001
loss_func = nn.BCEWithLogitsLoss()
# бинарная кросс-энтропия BCEWithLogitsLoss

epochs = 100 # число эпох обучения (в реальности нужно от 100 и более)
# переведите модель в режим обучения
model.train()
for _e in range(epochs):
    for x_train, y_train in train_data:
        predict = model(x_train.squeeze(0)).squeeze(-1)
        # вычислите прогноз модели для x_train
        loss = loss_func(predict, y_train.squeeze(0).float())
        # вычислите потери для predict и y_train
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # выполните один шаг обучения (градиентного спуска)

# переведите модель в режим эксплуатации
model.eval()
Q = 0 # начальное значение доли верных классификаций
for x_train, y_train in train_data:
    with torch.no_grad():
        p = torch.nn.functional.sigmoid(model(x_train.squeeze(0)).squeeze(-1)).round()
        # вычислите прогноз модели для x_train
        Q += torch.mean(torch.tensor(p == y_train.squeeze(0).float(), dtype=torch.float32)).item()
        # вычислите долю верных классификаций для p и y_train

Q = Q / len(d_train) # усреднение по всем батчам
print(Q)
